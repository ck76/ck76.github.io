**第一讲**

这一讲做了整体课程介绍，包括计算机视觉概述、历史背景以及课程组织等。



**第二讲**

图像分类：数据驱动方法、K-最近邻、线性分类I。



**第三讲**

损失函数和优化：包括线性分类II，高阶表征、图像特点，优化、随机梯度下降等内容。



**第四讲**

神经网络：介绍反向传播、多层感知器、神经元的概念。



**第五讲**

卷积神经网络：卷积神经网络的历史、卷积和池化、卷积神经网络的前瞻和愿景。



**第六讲**

训练神经网络（I）：包括激活函数、权重初始化、批量归一化等内容。



**第七讲**

训练神经网络（II）：优化方法、模型集成、正则化、数据扩张和迁移学习等。



**第八讲**

深度学习软件： Caffe、Torch、Theano、TensorFlow、Keras、**PyTorch**等。



**第九讲**

CNN架构：AlexNet，VGG，GoogLeNet，ResNet等。



**第十讲**

循环神经网络：主要包括RNN，LSTM，GRU，从语言建模、图像描述、视觉问答系统等方面进行描述。



**第十一讲**

检测和分割：语义分割、目标检测、实例分割。



**第十二讲**

可视化和理解：特征可视化和反演、对抗样本、DeepDream（深度梦想）和风格转移。



**第十三讲**

生成模型：PixelRNN / CNN 、变分自编码器、生成式对抗网络。



**第十四讲**

强化学习：策略梯度，hard attention模型；Q-Learning和 Actor-Critic学习。



**第十五讲**

现实世界使用：卷积算法，CPU / GPU、低精度模型压缩。



**第十六讲**

对抗性样本和对抗性训练：该部分由lan goodfellow主讲，详细讲解了对抗性样本和对抗性训练。



- https://blog.csdn.net/weixin_42247762/article/details/80467503
- [课程地址](http://cs231n.stanford.edu/2017/syllabus)